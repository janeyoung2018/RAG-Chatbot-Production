# RAG Chatbot Backend

FastAPI service orchestrating retrieval-augmented generation with Weaviate (or an in-memory fallback), Phoenix tracing, and product-aware enrichment.

## Requirements

- Python 3.11+
- `pip install -r backend/requirements.txt`
- Running Weaviate and Phoenix instances (Docker Compose spins these up automatically)

## Environment Variables

Environment is loaded from `.env`. Key settings:

- `OPENAI_API_KEY`: OpenAI (or OpenRouter) key to power embeddings/LLM.
- `OPENAI_API_BASE`: Override base URL (set to `https://openrouter.ai/api/v1` for OpenRouter).
- `LLM_MODEL`: Chat model (default `gpt-4o-mini`; try `meta-llama/llama-3.3-8b-instruct:free`).
- `EMBEDDINGS_MODEL`: Embedding model name (`text-embedding-3-small`).
- `WEAVIATE_URL` / `WEAVIATE_API_KEY`: Vector store connection.
- `PRODUCT_DATA_PATH`: Location of `products.jsonl` file.
- `API_KEY`: Required header (`X-API-Key`) for all non-health endpoints.
- `API_VERSION`: Reported via `/api/health`; set by Docker Compose with `BACKEND_VERSION`.
- `RATE_LIMIT_PER_MINUTE` / `RATE_LIMIT_WINDOW_SECONDS`: In-memory throttle for ingest/query endpoints.
- `PHOENIX_ENDPOINT`: Base URL for trace links returned to the frontend.

## Running Locally

```bash
conda create -n rag-backend python=3.11 -y  # skip if env exists
conda activate rag-backend
pip install -r backend/requirements.txt
uvicorn backend.app.main:app --host 0.0.0.0 --port 8000 --reload
```

If Weaviate is unavailable, the service falls back to an in-memory lexical store so local development and CI smoke tests still pass.

## Core Features

- **RAG Pipeline**: LangGraph-powered graph for retrieval and answer generation with Phoenix spans.
- **Product-Aware Context**: Merges catalog matches from `products.jsonl` with knowledge-base snippets.
- **Observability**: Phoenix `trace_id` and `trace_url` emitted with every `/api/query` response.
- **Security**: API key enforcement + configurable rate limiting on write/query routes.
- **Fallback Answering**: Deterministic summaries returned when no LLM credentials are configured.

## API Overview

- `GET /api/health` — Health information and pipeline readiness.
- `POST /api/ingest` — Ingest knowledge docs (requires `X-API-Key`).
- `POST /api/query` — Submit questions with optional `brand`, `category`, `tag`, `size` filters; returns answer, evidence, Phoenix trace metadata.
- `GET /api/products` — Filterable product catalog.
- `GET /api/products/{product_id}` — Specific product record.

Use `scripts/ingest.py --api-key $API_KEY` to feed documents and `scripts/ci_smoke.py` for quick regression checks.

## Testing & Linting

- `python -m compileall backend` — quick syntax verification (used in CI instructions).
- Integrate additional unit/integration tests via `pytest` (not bundled yet).

## Deployment Notes

- Docker Compose injects `BACKEND_VERSION` into `API_VERSION` and container labels for tracking.
- When deploying with OpenRouter, ensure both `OPENAI_API_BASE` and `OPENAI_API_KEY` point to OpenRouter credentials.
- Phoenix UI (default `http://localhost:6006`) surfaces spans generated by the backend for every ingest/query call.

